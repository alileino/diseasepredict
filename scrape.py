import string
import os
from lxml import html
import requests
from queue import Queue
from enum import Enum
from threading import Thread
import queue
import time
import bs4
import re

class JobType(Enum):
    SYMPTOM_LIST = 0,
    SYMPTOM = 1,
    CAUSES = 2,
    DISEASE = 3

class MayoBase:
    valid_chars = "-_.() %s%s" % (string.ascii_letters, string.digits)
    BASE_DIR = "mayo"
    @staticmethod
    def request(address, prefixDir):
        path = MayoBase.address_to_path(address, prefixDir)
        content = ""
        if os.path.exists(path) and os.stat(path).st_size != 0:
            with open(path, 'r') as f:
                content = f.read()

        else:
            print("Requesting:", address)
            content = MayoBase._request_address(address)


            with open(path, 'w', encoding="utf-8") as f:
                f.write(content)


        return content

    @staticmethod
    def _request_address(address):
        content = ""
        start = time.clock()
        timeout = 10
        while content == "" :
            page = requests.get(address)
            page.raise_for_status()
            content = page.content.decode("utf-8")
            if content == "" and time.clock() - timeout < start:
                os.sleep(1000)
                print("Retrying")
            else:
                break
        return content

    @staticmethod
    def address_to_path(address, prefixDir):
        dir = os.path.join(MayoBase.BASE_DIR, prefixDir)
        if not os.path.exists(dir):
            os.makedirs(dir)
        name = address.split('/')[-1]
        name = "".join(c for c in name if c in MayoBase.valid_chars)
        path = os.path.join(dir, name)
        return path

    @staticmethod
    def is_scraped(address, prefixDir):
        path = MayoBase.address_to_path(address, prefixDir)
        content = ""
        if os.path.exists(path):
            return True
        return False



class MayoWorker(Thread):
    '''
    '''
    BASE_ADDRESS = "http://www.mayoclinic.org"

    def __init__(self, queue):
        super(MayoWorker, self).__init__()
        self.queue = queue
        self.jobHandlers = {
            JobType.SYMPTOM_LIST: self.scrape_symptom_list,
            JobType.SYMPTOM : self.scrape_symptom,
            JobType.CAUSES : self.scrape_causes,
            JobType.DISEASE : self.scrape_disease
        }

    def run(self):
        while True:
            if self.queue.empty():
                print("Worker exited normally")
                return # todo: stopping indication
            try:
                job = self.queue.get()
                handler = self.jobHandlers[job["type"]]
                job["address"] = self.make_address(job["address"])
                handler( **job)
            except queue.Empty:
                return # todo: as above


    def scrape_symptom_list(self, **kwargs):
        '''
        Scrapes a list of symptoms on a page.
        :return: a list of new jobs generated by this scrape
        '''

        content = MayoBase.request(kwargs["address"], "symptom_list")

        tree = bs4.BeautifulSoup(content)
        symptomLinks = tree.select("#index > ol > li > a")
        for symptom in symptomLinks:
            href = symptom.attrs["href"]
            causesHref = href.replace("definition", "causes")
            self.queue.put({"type": JobType.SYMPTOM, "address" : href})
            self.queue.put({"type": JobType.CAUSES, "address" : causesHref})


    def scrape_symptom(self, **kwargs):
        content = MayoBase.request(kwargs["address"], "symptom")

    def scrape_causes(self, **kwargs):
        content = MayoBase.request(kwargs["address"], "symcauses")
        tree = bs4.BeautifulSoup(content, "lxml")
        causeLinks = tree.select("div#main-content > ol > li > a")
        for cause in causeLinks:
            # self.queue.put({"disease": JobType.})
            href = cause["href"]
            self.queue.put({"type" : JobType.DISEASE, "address" : href})

    def scrape_disease(self, **kwargs):
        content = MayoBase.request(kwargs["address"], "disease")
        tree = bs4.BeautifulSoup(content, "lxml")


    def make_address(self, *args):
        return MayoWorker.BASE_ADDRESS + "".join(args)

class MayoScraper(MayoBase):
    '''
    '''
    BASE_ADDRESS = "http://www.mayoclinic.org"
    LINKS_ADDRESS = "/symptoms/index?letter="
    def __init__(self):
        self.jobs = Queue()
        self.numWorkers = 1

    def scrape(self):
        for letter in string.ascii_uppercase:
            listAddress = MayoScraper.LINKS_ADDRESS + letter.lower()
            self.jobs.put({"type": JobType.SYMPTOM_LIST, "address": listAddress})
        self.start_workers()

    def start_workers(self):
        for i in range(self.numWorkers):
            worker = MayoWorker(self.jobs)
            worker.start()

def id_from_str(value):
    mo = re.search(r"([0-9]+)$", value)
    if mo == None:
        return None
    return mo.group(0)

def _collect_data(basedir):
    '''
    Collects the data from basedir in unprocessed form.
    :param basedir:
    :return: a list. Each entry is a tuple comprising of:
    id - the id of a symptom
    name - the name of a symptom (may contain a synonym in parentheses
    causes - a list of cause-tuples, each tuple comprising of
        id - the id of the cause
        name - the name of the cause (may contain a synonym in parentheses
    '''
    causespath = os.path.join(basedir, "symcauses")
    data = []
    for fn in os.listdir(causespath):
        path = os.path.join(causespath, fn)
        if os.path.isfile(path):

            with open(path, mode="r") as f:
                tree = bs4.BeautifulSoup(f, "lxml")

                causeLinks = tree.select("div#main-content > ol > li > a")
                causes = []
                for causel in causeLinks:
                    causeId = id_from_str(causel["href"])
                    causes.append((causeId, causel.getText().strip()))

                symptom = tree.select("div.headers > h1 > a")[0].getText()
                symptomId = id_from_str(fn)
                data.append((symptomId, symptom, causes))


    return data

def process_dataset(basedir):
    '''

    :param basedir: base dir to process. It should have subdirectories symptom and symcauses
    :return: a dictionary of disease-symptomlist pairs
    '''

    data = _collect_data(basedir)
    print(data)
    symptomCauses = {}





def main():
    process_dataset(MayoBase.BASE_DIR)


    m = MayoScraper()
    m.scrape()

if __name__ == "__main__":
    main()