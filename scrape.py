import string
import os
from lxml import html
import requests
from queue import Queue
from enum import Enum
from threading import Thread
import queue
import time
import bs4

class JobType(Enum):
    SYMPTOM_LIST = 0,
    SYMPTOM = 1,
    CAUSES = 2,
    DISEASE = 3

class MayoBase:
    valid_chars = "-_.() %s%s" % (string.ascii_letters, string.digits)
    BASE_DIR = "mayo"
    @staticmethod
    def request(address, prefixDir):
        path = MayoBase.address_to_path(address, prefixDir)
        content = ""
        if os.path.exists(path) and os.stat(path).st_size != 0:
            with open(path, 'r') as f:
                content = f.read()

        else:
            print("Requesting:", address)
            content = MayoBase._request_address(address)


            with open(path, 'w', encoding="utf-8") as f:
                f.write(content)


        return content

    @staticmethod
    def _request_address(address):
        content = ""
        start = time.clock()
        timeout = 10
        while content == "" :
            page = requests.get(address)
            page.raise_for_status()
            content = page.content.decode("utf-8")
            if content == "" and time.clock() - timeout < start:
                os.sleep(1000)
                print("Retrying")
            else:
                break
        return content

    @staticmethod
    def address_to_path(address, prefixDir):
        dir = os.path.join(MayoBase.BASE_DIR, prefixDir)
        if not os.path.exists(dir):
            os.makedirs(dir)
        name = address.split('/')[-1]
        name = "".join(c for c in name if c in MayoBase.valid_chars)
        path = os.path.join(dir, name)
        return path

    @staticmethod
    def is_scraped(address, prefixDir):
        path = MayoBase.address_to_path(address, prefixDir)
        content = ""
        if os.path.exists(path):
            return True
        return False



class MayoWorker(Thread):
    '''
    '''
    BASE_ADDRESS = "http://www.mayoclinic.org"

    def __init__(self, queue):
        super(MayoWorker, self).__init__()
        self.queue = queue
        self.jobHandlers = {
            JobType.SYMPTOM_LIST: self.scrape_symptom_list,
            JobType.SYMPTOM : self.scrape_symptom,
            JobType.CAUSES : self.scrape_causes,
            JobType.DISEASE : self.scrape_disease
        }

    def run(self):
        while True:
            if self.queue.empty():
                print("Worker exited normally")
                return # todo: stopping indication
            try:
                job = self.queue.get()
                handler = self.jobHandlers[job["type"]]
                job["address"] = self.make_address(job["address"])
                handler( **job)
            except queue.Empty:
                return # todo: as above


    def scrape_symptom_list(self, **kwargs):
        '''
        Scrapes a list of symptoms on a page.
        :return: a list of new jobs generated by this scrape
        '''

        content = MayoBase.request(kwargs["address"], "symptom_list")

        tree = bs4.BeautifulSoup(content)
        symptomLinks = tree.select("#index > ol > li > a")
        for symptom in symptomLinks:
            href = symptom.attrs["href"]
            causesHref = href.replace("definition", "causes")
            self.queue.put({"type": JobType.SYMPTOM, "address" : href})
            self.queue.put({"type": JobType.CAUSES, "address" : causesHref})


    def scrape_symptom(self, **kwargs):
        content = MayoBase.request(kwargs["address"], "symptom")

    def scrape_causes(self, **kwargs):
        content = MayoBase.request(kwargs["address"], "causes")
        tree = bs4.BeautifulSoup(content)
        causeLinks = tree.select("div#main-content > ol > li > a")
        for cause in causeLinks:
            # self.queue.put({"disease": JobType.})
            href = cause["href"]
            self.queue.put({"type" : JobType.DISEASE, "address" : href})

    def scrape_disease(self, **kwargs):
        content = MayoBase.request(kwargs["address"], "disease")
        tree = bs4.BeautifulSoup(content)


    def make_address(self, *args):
        return MayoWorker.BASE_ADDRESS + "".join(args)

class MayoScraper(MayoBase):
    '''
    '''
    BASE_ADDRESS = "http://www.mayoclinic.org"
    LINKS_ADDRESS = "/symptoms/index?letter="
    def __init__(self):
        self.jobs = Queue()
        self.numWorkers = 1

    def scrape(self):
        for letter in string.ascii_uppercase:
            listAddress = MayoScraper.LINKS_ADDRESS + letter.lower()
            self.jobs.put({"type": JobType.SYMPTOM_LIST, "address": listAddress})
        self.start_workers()

    def start_workers(self):
        for i in range(self.numWorkers):
            worker = MayoWorker(self.jobs)
            worker.start()


def process_dataset(basedir):
    '''

    :param basedir: base dir to process. It should have subdirectories symptom and causes
    :return: a dictionary of disease-symptomlist pairs
    '''
    pass




m = MayoScraper()
m.scrape()
# q = Queue()
# m = MayoSymptomWorker(q)
# m.start()
#
# page = requests.get("http://econpy.pythonanywhere.com/ex/001.html")
# tree=html.fromstring(page.content)
print()